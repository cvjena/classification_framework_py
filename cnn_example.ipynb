{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('/home/simon/Research/lib/caffe/python/')\n",
    "import caffe\n",
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "import Caffe\n",
    "import Resize\n",
    "import Pad\n",
    "import PIL\n",
    "import Classification\n",
    "import glob\n",
    "import pyprind\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "import Dataset\n",
    "import Evaluation\n",
    "import ImageReader\n",
    "import Crop\n",
    "import Classificator\n",
    "import sklearn\n",
    "import scipy\n",
    "import Lambda\n",
    "import Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # Read dataset from image, label, and split list\n",
    "    basedir = '/home/simon/Datasets/CUB_200_2011/'\n",
    "    d = Dataset.Dataset()\n",
    "    d.read_from_file('%s/imagelist_absolute.txt'%basedir, 'imagepaths','string')\n",
    "    d.read_from_file('%s/labels.txt'%basedir,'labels','int')\n",
    "    d.read_from_file('%s/tr_ID.txt'%basedir,'split_assignments','int')\n",
    "else:\n",
    "    # Read dataset from directory structure\n",
    "    d = Dataset.Dataset()\n",
    "    d.use_images_in_folder('/home/simon/Datasets/dtd/images/')\n",
    "    d.create_labels_from_path()\n",
    "    d.make_random_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Classification.Classification()\n",
    "c.add_algorithm(ImageReader.ImageReader())\n",
    "s = 224\n",
    "#c.add_algorithm(Resize.Resize((s,s), mode='stretch'))\n",
    "c.add_algorithm(Resize.Resize((s,), mode='resize_smaller_side'))\n",
    "c.add_algorithm(Crop.Crop((s,s),'center'))\n",
    "#c.add_algorithm(FlipAugmentation())\n",
    "#c.add_algorithm(Pad.Pad(s,s))\n",
    "modeldir = '/home/simon/Data/caffe/alexnet/'\n",
    "c.add_algorithm(Caffe.Caffe(modeldir + '/deploy.prototxt',\n",
    "                           modeldir + '/model',\n",
    "                           #mean = np.float32([0,0,0]),\n",
    "                           outblob = 'fc7',\n",
    "                           batchsize = 5))\n",
    "\n",
    "c.add_algorithm(Lambda.Lambda(lambda x: sklearn.preprocessing.normalize(x.reshape((1,-1)))))\n",
    "c.add_algorithm(Classificator.Classificator(sklearn.svm.LinearSVC(C=1),False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    ev = Evaluation.Evaluation()\n",
    "    ev.fixed_split_eval(d, c)\n",
    "elif True:\n",
    "    ev = Evaluation.Evaluation()\n",
    "    ev.random_split_eval(d,c,runs=1,absolute_train_per_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = np.where(d.split_assignments)[0]\n",
    "test_ids = np.where(np.invert(np.array(d.split_assignments).astype(bool)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calc the features\n",
    "def calc_features(pipeline, images):\n",
    "    out_shape = (len(images), ) + pipeline.compute(images[0]).shape\n",
    "    feats = np.zeros(tuple(out_shape),dtype=np.float32)\n",
    "    feats[:] = 1\n",
    "    print('feats will occupy %.4f GiB'%(feats.nbytes/1024/1024/1024))\n",
    "    for idx,impath in enumerate(pyprind.prog_bar(images, update_interval=2)):\n",
    "        feats[idx,...] = c.compute(impath)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_fv = calc_features(c, np.array(d.imagepaths)[train_ids])\n",
    "test_fv = calc_features(c, np.array(d.imagepaths)[test_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fv = calc_features(c, np.array(d.imagepaths)[test_ids][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = dict(train_feats=train_fv.reshape((len(train_fv),-1)),\n",
    "                                test_feats=test_fv.reshape((len(test_fv),-1)),\n",
    "                                train_Y=np.array(d.labels)[train_ids],\n",
    "                                test_Y=np.array(d.labels)[test_ids])\n",
    "\n",
    "if False:\n",
    "    scaler = sklearn.preprocessing.RobustScaler().fit(task['train_feats'])\n",
    "    task['train_feats'] = scaler.transform(task['train_feats'])\n",
    "    task['test_feats'] = scaler.transform(task['test_feats'])\n",
    "elif False:\n",
    "    task['train_feats'] = sklearn.preprocessing.normalize(task['train_feats'])\n",
    "    task['test_feats'] = sklearn.preprocessing.normalize(task['test_feats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comparison: explicit feature transform of linear kernel\n",
    "model = sklearn.svm.LinearSVC(C=1)\n",
    "model.fit(task['train_feats'],task['train_Y'])\n",
    "model.score(task['test_feats'],task['test_Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "import sklearn.metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calc the features\n",
    "def calc_features(pipeline, images):\n",
    "    out_shape = (len(images), ) + pipeline.compute(images[0]).shape\n",
    "    feats = np.zeros(tuple(out_shape),dtype=np.float32)\n",
    "    feats[:] = 1\n",
    "    print('feats will occupy %.4f GiB'%(feats.nbytes/1024/1024/1024))\n",
    "    for idx,impath in enumerate(pyprind.prog_bar(images, update_interval=2)):\n",
    "        feats[idx,...] = c.compute(impath)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats = calc_features(c, d.imagepaths)\n",
    "all_feats = all_feats.reshape((len(d.labels),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.svm.LinearSVC(C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(estimator, X, y):\n",
    "        cm = sklearn.metrics.confusion_matrix(y, estimator.predict(X))\n",
    "        acc = cm.diagonal().sum()/cm.ravel().sum()\n",
    "        cm = cm / cm.sum(axis=1,keepdims=True)\n",
    "        mAP = np.nanmean(cm.diagonal())\n",
    "        return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i in pyprind.prog_bar(range(100)):\n",
    "    d.split_assignments = []\n",
    "    d.fill_split_assignments(0)\n",
    "    d.make_random_split(absolute_per_class=1, source_value=0, target_value=1)\n",
    "    train_ids = np.where(d.split_assignments)[0]\n",
    "    test_ids = np.where(np.invert(np.array(d.split_assignments).astype(bool)))[0]\n",
    "    model.fit(all_feats[train_ids], np.array(d.labels)[train_ids])\n",
    "    scores.append(compute_score(model, all_feats[test_ids], np.array(d.labels)[test_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = [im for im,split in zip(d.imagepaths,d.split_assignments) if split==0]\n",
    "test_labels = [label for label,split in zip(d.labels,d.split_assignments) if split==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = c.compute_all(test_images)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
